# Adam optimizer configuration

name: "adam"  # Options: adam, adamw, sgd
learning_rate: 1e-3
weight_decay: 0.0